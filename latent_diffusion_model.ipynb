{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19034d84-0e3a-4c72-8f70-1ec8c42c49fb",
      "metadata": {
        "id": "19034d84-0e3a-4c72-8f70-1ec8c42c49fb"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/12_gan/latent_diffusion_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e234724-4d59-4a19-bfbf-8683caf72004",
      "metadata": {
        "id": "4e234724-4d59-4a19-bfbf-8683caf72004",
        "tags": []
      },
      "source": [
        "# Latent Diffusion Model (LDM)\n",
        "\n",
        "---\n",
        "## 目的\n",
        "Pytorchを用いてLatent Diffusion Model (LDM) を構築し，画像の生成を行う．"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6750088d-2756-4f48-bb20-61cb3f27dd13",
      "metadata": {
        "id": "6750088d-2756-4f48-bb20-61cb3f27dd13",
        "tags": []
      },
      "source": [
        "## モジュールのインポート\n",
        "\n",
        "はじめに必要となるモジュールをインポートします．\n",
        "\n",
        "### GPUの確認\n",
        "GPUを使用した計算が可能かどうかを確認します．\n",
        "\n",
        "`GPU availability: True`と表示されれば，GPUを使用した計算を行うことが可能です．\n",
        "Falseとなっている場合は，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b49050-ffb4-4749-b8c9-1ffe9e46155f",
      "metadata": {
        "id": "34b49050-ffb4-4749-b8c9-1ffe9e46155f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from einops import einsum\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be86fdf3-d540-4a79-8cfe-d1f5681ba43f",
      "metadata": {
        "id": "be86fdf3-d540-4a79-8cfe-d1f5681ba43f",
        "tags": []
      },
      "source": [
        "## ネットワークの構築\n",
        "LDMは医療用画像のセマンティックセグメンテーションを行うモデルとして提案されたU-Netと，離散的なベクトルを扱う生成モデルとして提案されたVector Quantised Variational AutoEncoder (VQ-VAE) をベースにネットワークを構築しています．\n",
        "\n",
        "Denoising Diffusion Probabilistic Model (DDPM) ではRGB画像上でデノイジング処理をしていました．しかし，画像の解像度に比例して計算量も増加するという問題がDDPMには存在していたため，LDMではVQ-VAEを用いて高次元なデータである画像を低次元なデータである特徴量に圧縮し，その特徴量に対してデノイジング処理をしています．また，テキストやセマンティックマスク画像などの特徴量をU-Netに与えることで条件付け生成を可能としています．そのためLDMには，実行したいタスクに合わせて条件付けエンコーダ（TransformerやCLIP，VAEなど）が追加されることがあります．\n",
        "\n",
        "LDMのネットワーク構造は以下の点でDDPMとは異なります．\n",
        "* VQ-VAEの追加\n",
        "* 条件付け用エンコーダの追加（必要に応じて）"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60bf80ed-68ed-40dc-9966-36b13a800f7c",
      "metadata": {
        "id": "60bf80ed-68ed-40dc-9966-36b13a800f7c",
        "tags": []
      },
      "source": [
        "## Position Embeddings\n",
        "LDMでは各時刻$t$のノイズを推定する時，ネットワークのパラメータは共通です．時刻$t$ごとにネットワークを構築するのではなく，どの時刻$t$かを表す情報をネットワークに与えることで各時刻$t$のノイズを推定することが可能となります．\n",
        "\n",
        "Position Embeddingsでは以下に示す式によって時刻の特徴量を求め，U-Net層の各Residual blockに追加されます．\n",
        "$$\n",
        "PE_{(pos, 2i)} = \\sin (\\frac{pos}{10000^{(2i/d)}})\n",
        "$$\n",
        "$$\n",
        "PE_{(pos, 2i+1)} = \\cos (\\frac{pos}{10000^{(2i/d)}})\n",
        "$$\n",
        "ここで，$pos$は時刻，$i$は時刻特徴量の次元のインデックス，$d$は時刻特徴量の次元数を表します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cb2b8b6d-9d32-4928-a4ee-cf7e74d52871",
      "metadata": {
        "id": "cb2b8b6d-9d32-4928-a4ee-cf7e74d52871",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def PositionEmbeddings(time_steps, temb_dim):\n",
        "    factor = 10000 ** ((torch.arange(start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2)))\n",
        "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
        "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
        "    return t_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b8aa5cc-bf80-49f1-aab9-5f6f7b40746e",
      "metadata": {
        "id": "4b8aa5cc-bf80-49f1-aab9-5f6f7b40746e",
        "tags": []
      },
      "source": [
        "## ResNet Block\n",
        "ResNetは，通常のネットワークのように，何かしらの処理ブロックによる変換$F(x)$を単純に次の層に渡していくのではなく，残差接続構造によりその処理ブロックへの入力$x$をショートカットし， $H(x) = F(x)+x$を次の層に渡すようにしています．残差接続構造により，誤差逆伝播時に勾配が消失しても，層をまたいで値を伝播することができます．このショートカットを含めた処理単位をResidual blockと呼びます．\n",
        "\n",
        "LDMのResNet Blockでは，Position Embeddingsで求めた時刻特徴量や必要に応じて条件付け特徴量を画像特徴量に追加し，残差接続を行います．ここで効率的な学習のために活性化関数にはRectified Linear Unit (ReLU)関数ではなく，Sigmoid-weighted Linear Unit (SiLU) 関数を用います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0941f9cc-04f3-446e-afc1-f8ea121e32a7",
      "metadata": {
        "id": "0941f9cc-04f3-446e-afc1-f8ea121e32a7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ダウンサンプルブロック\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, down_sample, num_heads, num_layers, attn, norm_channels):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.down_sample = down_sample\n",
        "        self.attn = attn\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "        self.resnet_conv_first = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                ) for i in range(num_layers)])\n",
        "        if self.t_emb_dim is not None:\n",
        "            self.t_emb_layers = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.SiLU(),\n",
        "                    nn.Linear(self.t_emb_dim, out_channels)\n",
        "                ) for _ in range(num_layers)])\n",
        "        self.resnet_conv_second = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                ) for _ in range(num_layers)])\n",
        "\n",
        "        if self.attn:\n",
        "            self.attention_norms = nn.ModuleList([nn.GroupNorm(norm_channels, out_channels)\n",
        "                 for _ in range(num_layers)])\n",
        "            self.attentions = nn.ModuleList(\n",
        "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                 for _ in range(num_layers)])\n",
        "\n",
        "        self.residual_input_conv = nn.ModuleList([\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers)])\n",
        "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels, 4, 2, 1) if self.down_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, t_emb=None):\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            if self.t_emb_dim is not None:\n",
        "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "\n",
        "            if self.attn:\n",
        "                batch_size, channels, h, w = out.shape\n",
        "                in_attn = out.reshape(batch_size, channels, h * w)\n",
        "                in_attn = self.attention_norms[i](in_attn)\n",
        "                in_attn = in_attn.transpose(1, 2)\n",
        "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "                out = out + out_attn\n",
        "        out = self.down_sample_conv(out)\n",
        "        return out\n",
        "\n",
        "# 中間ブロック\n",
        "class MidBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "        self.resnet_conv_first = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                ) for i in range(num_layers + 1)])\n",
        "        if self.t_emb_dim is not None:\n",
        "            self.t_emb_layers = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.SiLU(),\n",
        "                    nn.Linear(t_emb_dim, out_channels)\n",
        "                ) for _ in range(num_layers + 1)])\n",
        "        self.resnet_conv_second = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                ) for _ in range(num_layers + 1)])\n",
        "\n",
        "        self.attention_norms = nn.ModuleList(\n",
        "            [nn.GroupNorm(norm_channels, out_channels)\n",
        "             for _ in range(num_layers)])\n",
        "\n",
        "        self.attentions = nn.ModuleList(\n",
        "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "             for _ in range(num_layers)])\n",
        "        self.residual_input_conv = nn.ModuleList([\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers + 1)])\n",
        "\n",
        "    def forward(self, x, t_emb=None):\n",
        "        out = x\n",
        "        resnet_input = out\n",
        "        out = self.resnet_conv_first[0](out)\n",
        "        if self.t_emb_dim is not None:\n",
        "            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
        "        out = self.resnet_conv_second[0](out)\n",
        "        out = out + self.residual_input_conv[0](resnet_input)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i + 1](out)\n",
        "            if self.t_emb_dim is not None:\n",
        "                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i + 1](out)\n",
        "            out = out + self.residual_input_conv[i + 1](resnet_input)\n",
        "\n",
        "        return out\n",
        "\n",
        "# アップサンプルブロック\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample, num_heads, num_layers, attn, norm_channels, model_type):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.up_sample = up_sample\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "        self.attn = attn\n",
        "        self.model_type = model_type\n",
        "        self.resnet_conv_first = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                ) for i in range(num_layers)])\n",
        "\n",
        "        if self.t_emb_dim is not None:\n",
        "            self.t_emb_layers = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.SiLU(),\n",
        "                    nn.Linear(t_emb_dim, out_channels)\n",
        "                ) for _ in range(num_layers)])\n",
        "\n",
        "        self.resnet_conv_second = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(norm_channels, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                ) for _ in range(num_layers)])\n",
        "        if self.attn:\n",
        "            self.attention_norms = nn.ModuleList([\n",
        "                    nn.GroupNorm(norm_channels, out_channels)\n",
        "                    for _ in range(num_layers)])\n",
        "\n",
        "            self.attentions = nn.ModuleList([\n",
        "                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                    for _ in range(num_layers)])\n",
        "\n",
        "        self.residual_input_conv = nn.ModuleList([\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers)])\n",
        "        if self.model_type == 'unet':\n",
        "            self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, 4, 2, 1) if self.up_sample else nn.Identity()\n",
        "        else:\n",
        "            self.up_sample_conv = nn.ConvTranspose2d(in_channels, in_channels, 4, 2, 1) if self.up_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, out_down=None, t_emb=None):\n",
        "        x = self.up_sample_conv(x)\n",
        "        if out_down is not None:\n",
        "            x = torch.cat([x, out_down], dim=1)\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            if self.t_emb_dim is not None:\n",
        "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "\n",
        "            if self.attn:\n",
        "                batch_size, channels, h, w = out.shape\n",
        "                in_attn = out.reshape(batch_size, channels, h * w)\n",
        "                in_attn = self.attention_norms[i](in_attn)\n",
        "                in_attn = in_attn.transpose(1, 2)\n",
        "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "                out = out + out_attn\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc850fb5-6a45-4598-8eee-a479908649cd",
      "metadata": {
        "id": "cc850fb5-6a45-4598-8eee-a479908649cd"
      },
      "source": [
        "## VQ-VAE\n",
        "VQ-VAEは，離散的な潜在空間を持ったVariational Autoencoder (VAE) です．Encoderにより高次元な入力データを低次元な特徴量（潜在変数）に圧縮します．そして，コードブックと呼ばれる事前に用意した離散的なベクトルと潜在変数の距離を計算し，潜在変数を最も距離が近いコードブックに置換することで離散的な潜在編巣を獲得します．最後に，Decoderにより離散的な潜在変数から入力データを再構成します．\n",
        "\n",
        "LDMでは，VQ-VAEのEncoderで圧縮・離散化された潜在変数をU-Netに入力し，U-Netでノイズ除去されたデータをVQ-VAEのDecoderで再構成しています．"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d88760c-288b-4eae-86e0-b69d6248a9ba",
      "metadata": {
        "id": "7d88760c-288b-4eae-86e0-b69d6248a9ba"
      },
      "source": [
        "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3916683/fd7c0c34-9dfd-4a7d-5b32-8394e9ccf716.png\" width=60%>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "de0acad8-11f5-4b42-8779-ff1e91892a66",
      "metadata": {
        "id": "de0acad8-11f5-4b42-8779-ff1e91892a66",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, im_channels):\n",
        "        super().__init__()\n",
        "        # エンコーダ\n",
        "        self.encoder_conv_in = nn.Conv2d(im_channels, 32, kernel_size=3, padding=(1, 1))\n",
        "        self.encoder_layers = nn.ModuleList([])\n",
        "        self.encoder_layers.append(DownBlock(32, 64, t_emb_dim=None, down_sample=True, num_heads=16, num_layers=1, attn=False, norm_channels=32))\n",
        "        self.encoder_layers.append(DownBlock(64, 128, t_emb_dim=None, down_sample=True, num_heads=16, num_layers=1, attn=False, norm_channels=32))\n",
        "        self.encoder_mids = MidBlock(128, 128, t_emb_dim=None, num_heads=16, num_layers=1, norm_channels=32)\n",
        "        self.encoder_norm_out = nn.GroupNorm(32, 128)\n",
        "        self.encoder_conv_out = nn.Conv2d(128, 3, kernel_size=3, padding=1)\n",
        "        # 量子化\n",
        "        self.pre_quant_conv = nn.Conv2d(3, 3, kernel_size=1)\n",
        "        self.embedding = nn.Embedding(20, 3)\n",
        "        self.post_quant_conv = nn.Conv2d(3, 3, kernel_size=1)\n",
        "        # デコーダ\n",
        "        self.decoder_conv_in = nn.Conv2d(3, 128, kernel_size=3, padding=(1, 1))\n",
        "        self.decoder_mids = MidBlock(128, 128, t_emb_dim=None, num_heads=16, num_layers=1, norm_channels=32)\n",
        "        self.decoder_layers = nn.ModuleList([])\n",
        "        self.decoder_layers.append(UpBlock(128, 64, t_emb_dim=None, up_sample=True, num_heads=16, num_layers=1, attn=False, norm_channels=32, model_type='vqvae'))\n",
        "        self.decoder_layers.append(UpBlock(64, 32, t_emb_dim=None, up_sample=True, num_heads=16, num_layers=1, attn=False, norm_channels=32, model_type='vqvae'))\n",
        "        self.decoder_norm_out = nn.GroupNorm(32, 32)\n",
        "        self.decoder_conv_out = nn.Conv2d(32, im_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def quantize(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = x.reshape(x.size(0), -1, x.size(-1))\n",
        "        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n",
        "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
        "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n",
        "        x = x.reshape((-1, x.size(-1)))\n",
        "        commmitment_loss = torch.mean((quant_out.detach() - x) ** 2)\n",
        "        codebook_loss = torch.mean((quant_out - x.detach()) ** 2)\n",
        "        quantize_losses = {\n",
        "            'codebook_loss': codebook_loss,\n",
        "            'commitment_loss': commmitment_loss\n",
        "        }\n",
        "        quant_out = x + (quant_out - x).detach()\n",
        "        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)\n",
        "        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n",
        "        return quant_out, quantize_losses, min_encoding_indices\n",
        "\n",
        "    def encode(self, x):\n",
        "        out = self.encoder_conv_in(x)\n",
        "        for down in self.encoder_layers:\n",
        "            out = down(out)\n",
        "        out = self.encoder_mids(out)\n",
        "        out = self.encoder_norm_out(out)\n",
        "        out = nn.SiLU()(out)\n",
        "        out = self.encoder_conv_out(out)\n",
        "        out = self.pre_quant_conv(out)\n",
        "        out, quant_losses, _ = self.quantize(out)\n",
        "        return out, quant_losses\n",
        "\n",
        "    def decode(self, z):\n",
        "        out = z\n",
        "        out = self.post_quant_conv(out)\n",
        "        out = self.decoder_conv_in(out)\n",
        "        out = self.decoder_mids(out)\n",
        "        for up in self.decoder_layers:\n",
        "            out = up(out)\n",
        "        out = self.decoder_norm_out(out)\n",
        "        out = nn.SiLU()(out)\n",
        "        out = self.decoder_conv_out(out)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, quant_losses = self.encode(x)\n",
        "        out = self.decode(z)\n",
        "        return out, z, quant_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e9f226b-d3b6-4749-8d92-db7e6522e9cf",
      "metadata": {
        "id": "7e9f226b-d3b6-4749-8d92-db7e6522e9cf",
        "tags": []
      },
      "source": [
        "## Unet\n",
        "LDMのU-Netでは，ノイズが付与された画像特徴量のバッチとそれぞれのノイズレベル（時刻$t$），条件付け特徴量を入力として受け取り，画像特徴量に追加されたノイズを推定しています．"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "665b0c9f-17b5-4c14-99cd-2acf05281541",
      "metadata": {
        "id": "665b0c9f-17b5-4c14-99cd-2acf05281541"
      },
      "source": [
        "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3916683/98558f2e-fd55-ba8d-4863-2fe4661198a4.png\" width=100%>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "684afcb5-f8dc-40ee-bb17-9e2b2463f3b3",
      "metadata": {
        "id": "684afcb5-f8dc-40ee-bb17-9e2b2463f3b3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(self, im_channels):\n",
        "        super().__init__()\n",
        "        self.class_emb = nn.Embedding(10, 256)\n",
        "        self.conv_in = nn.Conv2d(im_channels, 128, kernel_size=3, padding=1)\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(256, 256)\n",
        "        )\n",
        "        # ダウンサンプル\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.downs.append(DownBlock(128, 256, t_emb_dim=256, down_sample=False, num_heads=16, num_layers=2, attn=True, norm_channels=32))\n",
        "        self.downs.append(DownBlock(256, 256, t_emb_dim=256, down_sample=False, num_heads=16, num_layers=2, attn=True, norm_channels=32))\n",
        "        self.downs.append(DownBlock(256, 256, t_emb_dim=256, down_sample=False, num_heads=16, num_layers=2, attn=True, norm_channels=32))\n",
        "        # 中間\n",
        "        self.mids = MidBlock(256, 256, t_emb_dim=256, num_heads=16, num_layers=2, norm_channels=32)\n",
        "        # アップサンプル\n",
        "        self.ups = nn.ModuleList([])\n",
        "        self.ups.append(UpBlock(256 * 2, 256, t_emb_dim=256, up_sample=False, num_heads=16, num_layers=2, attn=True, norm_channels=32, model_type='unet'))\n",
        "        self.ups.append(UpBlock(256 * 2, 128, t_emb_dim=256, up_sample=False, num_heads=16, num_layers=2, attn=True, norm_channels=32, model_type='unet'))\n",
        "        self.ups.append(UpBlock(128 * 2, 128, t_emb_dim=256, up_sample=False, num_heads=16, num_layers=2, attn=True, norm_channels=32, model_type='unet'))\n",
        "        self.norm_out = nn.GroupNorm(32, 128)\n",
        "        self.conv_out = nn.Conv2d(128, im_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, t, cond_input=None):\n",
        "        out = self.conv_in(x)\n",
        "        t_emb = PositionEmbeddings(torch.as_tensor(t).long(), 256)\n",
        "        t_emb = self.t_proj(t_emb)\n",
        "        class_embed = einsum(cond_input.float(), self.class_emb.weight, 'b n, n d -> b d')\n",
        "        t_emb += class_embed\n",
        "        down_outs = []\n",
        "\n",
        "        for idx, down in enumerate(self.downs):\n",
        "            down_outs.append(out)\n",
        "            out = down(out, t_emb)\n",
        "\n",
        "        out = self.mids(out, t_emb)\n",
        "\n",
        "        for up in self.ups:\n",
        "            down_out = down_outs.pop()\n",
        "            out = up(out, down_out, t_emb)\n",
        "        out = self.norm_out(out)\n",
        "        out = nn.SiLU()(out)\n",
        "        out = self.conv_out(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d77a4f10-d303-479c-8abf-a6ec59994e90",
      "metadata": {
        "id": "d77a4f10-d303-479c-8abf-a6ec59994e90",
        "tags": []
      },
      "source": [
        "## Forward Process\n",
        "Forward Processは，入力画像$\\mathbf{x}_0$に対してノイズを付与し最終的には完全なノイズ$\\mathbf{x}_T$へと変換する確率過程であり，以下に示す式のように正規分布に従うマルコフ過程で定義されます．\n",
        "$$\n",
        "q(\\mathbf{x}_{1:T} | \\mathbf{x}_0) = \\prod_{t=1}^T q(\\mathbf{x}_t | \\mathbf{x}_{t-1})\n",
        "$$\n",
        "$$\n",
        "q(\\mathbf{x}_t |  \\mathbf{x}_{t-1}) = N(\\mathbf{x}_t ; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1} \\mathbf{I})\n",
        "$$\n",
        "ここで，$\\beta_t$は変化量を表すパラメータを表します．Forward Processでは学習を行わず，単純に連続的な微小変化によって解析可能な分布に変換することが目的です．ノイズ量を調整するスケジューラの値は$0.0015 \\sim 0.0195$，ノイズを付与するステップは$1000$回とします．"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "286f6064-ef65-49d0-9139-fe5cb20b2564",
      "metadata": {
        "id": "286f6064-ef65-49d0-9139-fe5cb20b2564"
      },
      "source": [
        "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3916683/44f8adec-7e88-557e-125d-3c5148616cc8.png\" width=100%>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a25735e9-195f-4240-8d7e-3557f50cacac",
      "metadata": {
        "id": "a25735e9-195f-4240-8d7e-3557f50cacac",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# スケジューラ\n",
        "def linear_beta_schedule(timesteps):\n",
        "    beta_start = 0.0015\n",
        "    beta_end = 0.0195\n",
        "    return torch.linspace(beta_start ** 0.5, beta_end ** 0.5, timesteps) ** 2\n",
        "\n",
        "# ステップ数\n",
        "num_timesteps = 1000\n",
        "\n",
        "betas = linear_beta_schedule(timesteps=num_timesteps)\n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    out = a.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
        "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "682790c2-0f09-427d-ab55-e2e40cc25dae",
      "metadata": {
        "id": "682790c2-0f09-427d-ab55-e2e40cc25dae",
        "tags": []
      },
      "source": [
        "## Reverse Process\n",
        "変化量を表すパラメータ$\\beta_t$が十分に小さい連続変換（Forward Process）の場合，その逆変換（Reverse Process）は同じ関数系で表現することが可能であり，ガウスノイズの除去として考えることができます．そのため，Reverse Processは以下に示す式のように定義します．\n",
        "$$\n",
        "p_\\theta (\\mathbf{x}_{0:T}) = p_\\theta (\\mathbf{x}_T) \\prod_{t=1}^T p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_t)\n",
        "$$\n",
        "$$\n",
        "p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_t) = N( \\mathbf{x}_{t-1}; \\mathbf{\\mu}_\\theta (\\mathbf{x}_t, t), \\Sigma_\\theta(\\mathbf{x}_t, t))\n",
        "$$\n",
        "上記の式では，平均$\\mathbf{\\mu}_\\theta$と共分散$\\Sigma_\\theta(\\mathbf{x}_t, t)$をニューラルネットワークで学習することになっていますが，論文では共分散$\\Sigma_\\theta(\\mathbf{x}_t, t)$をあらかじめ固定し学習しません．そのため，本ノートブックでは共分散を固定して実装していますが，ステップ数$t$を減らした場合においては平均と共分散の両方を学習した方が良いです．"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "921db54c-0404-4ceb-80f3-4cd4ece608de",
      "metadata": {
        "id": "921db54c-0404-4ceb-80f3-4cd4ece608de"
      },
      "source": [
        "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3916683/68d24842-f3bc-0905-9588-9b279365484a.png\" width=100%>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b45328cf-d86e-49bb-9cd7-e9c330f094fe",
      "metadata": {
        "id": "b45328cf-d86e-49bb-9cd7-e9c330f094fe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(noise_pred, x, t, t_index, cond_input):\n",
        "    betas_t = extract(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
        "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "\n",
        "    model_mean = sqrt_recip_alphas_t * (x - betas_t * noise_pred / sqrt_one_minus_alphas_cumprod_t)\n",
        "\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "        noise = torch.randn_like(x)\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, vae, cond_input, shape):\n",
        "    device = next(model.parameters()).device\n",
        "    b = shape[0]\n",
        "    xt = torch.randn(shape, device=device)\n",
        "    imgs = []\n",
        "\n",
        "    for i in tqdm(reversed(range(0, num_timesteps)), desc='sampling loop time step', total=num_timesteps):\n",
        "        t = (torch.ones((xt.shape[0],))*i).long().to(device)\n",
        "        noise_pred = model(xt, t, cond_input)\n",
        "        xt = p_sample(noise_pred, xt, torch.full((b,), i, device=device, dtype=torch.long), i, cond_input)\n",
        "        im = vae.decode(xt)\n",
        "        im = torch.clamp(im, -1., 1.).detach().cpu()\n",
        "        im = (im + 1) / 2\n",
        "        imgs.append(im)\n",
        "    return imgs\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, vae, cond_input, image_size, batch_size=16, channels=3):\n",
        "    return p_sample_loop(model, vae, cond_input, shape=(batch_size, channels, image_size, image_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1406ee24-e36a-44a6-8171-012bc764e6e7",
      "metadata": {
        "id": "1406ee24-e36a-44a6-8171-012bc764e6e7",
        "tags": []
      },
      "source": [
        "## 損失関数\n",
        "LDMでは，VAEと同様にデータ$\\mathbf{x}_0$の対数尤度の変分下限を最大化するように学習します．LDMの損失関数は以下に示す式のようになります．\n",
        "$$\n",
        "\\mathcal{L}_{\\rm{simple}} = \\mathbb{E}_{t, \\mathbf{x}_0, \\epsilon, y} [\\parallel \\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{a}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{a}_t}\\epsilon, t, y) \\parallel^2]\n",
        "$$\n",
        "ここで，$\\mathbf{x}_t = \\sqrt{\\bar{a}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{a}_t}\\epsilon$と表せるため，損失関数は以下のようになります．\n",
        "$$\n",
        "\\mathcal{L}_{\\rm{simple}} = \\mathbb{E}_{t, \\mathbf{x}_0, \\epsilon,y } [\\parallel \\epsilon - \\epsilon_\\theta(\\mathbf{x}_t, t, y) \\parallel^2]\n",
        "$$\n",
        "従ってLDMでは，ノイズが付与された画像特徴量$\\mathbf{x}_t$と時刻$t$，条件付け特徴量$y$が入力され，付与されたノイズ$\\epsilon$を推定するニューラルネットワーク$\\epsilon_\\theta$を学習します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a1b4d07e-7a4c-4161-9a88-11e6e70a5f3f",
      "metadata": {
        "id": "a1b4d07e-7a4c-4161-9a88-11e6e70a5f3f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def p_losses(denoise_model, x_start, t, y, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
        "    predicted_noise = denoise_model(x_noisy, t, cond_input=y)\n",
        "    loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a67799d-7b6b-41d2-8078-52ed71a08134",
      "metadata": {
        "id": "7a67799d-7b6b-41d2-8078-52ed71a08134",
        "tags": []
      },
      "source": [
        "## VQ-VAEの学習\n",
        "\n",
        "本来であれば，VQ-VAEを任意のデータセットで学習するのですが，今回は事前に学習したVQ-VAEの重みを用いてLDMを学習します．\n",
        "ダウンロードするVQ-VAEは，MNISTデータセットを用いて学習したものです． 画像サイズは28×28です． 以下のリンクからpretrainモデルのzipファイルをダウンロードし，解凍をします． 中にはVQ-VAEのパラメータvqvae_autoencoder_ckpt.pthが入っています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21821bae-3eab-4a83-a959-f054aeff53fe",
      "metadata": {
        "id": "21821bae-3eab-4a83-a959-f054aeff53fe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1BuHdwkdqZxP9zstWodnVxTOsURk-Vf7z' -O ckpt.zip\n",
        "!unzip -q -o ckpt.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d7e235-f268-44d8-ab49-424facc5fb20",
      "metadata": {
        "id": "e4d7e235-f268-44d8-ab49-424facc5fb20"
      },
      "source": [
        "## データセット，最適化関数などの設定\n",
        "データセットはMNISTを用いて学習をします．このとき，LDMのU-Netを学習させるためにVQ-VAEのパラメータは固定します．\n",
        "最適化関数にはAdam Optimizerを使用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a72d94-32f0-4cbe-99a4-3645903e5835",
      "metadata": {
        "id": "d5a72d94-32f0-4cbe-99a4-3645903e5835",
        "tags": []
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([transforms.ToTensor()])\n",
        "mnist_data = datasets.MNIST(root='./data', train=True, transform=transform_train, download=True)\n",
        "train_loader = DataLoader(dataset=mnist_data, batch_size=128, shuffle=True)\n",
        "\n",
        "image_size = 28\n",
        "image_channels = 1\n",
        "\n",
        "num_epochs = 6\n",
        "num_classes = 10\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "vae = VQVAE(im_channels=image_channels).to(device)\n",
        "vae.eval()\n",
        "vae.load_state_dict(torch.load(os.path.join('./ckpt/vqvae_autoencoder_ckpt.pth'),map_location=device))\n",
        "for param in vae.parameters():\n",
        "    param.requires_grad = False\n",
        "print('vq-vae param freezed')\n",
        "\n",
        "model = Unet(im_channels=3).to(device)\n",
        "optimizer = Adam(model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e07fb4-07c7-4ab1-89d1-5eb798e8b055",
      "metadata": {
        "id": "62e07fb4-07c7-4ab1-89d1-5eb798e8b055"
      },
      "source": [
        "## ネットワークの学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15f3547c-d736-4dae-b017-0d5ccdbbbdab",
      "metadata": {
        "id": "15f3547c-d736-4dae-b017-0d5ccdbbbdab",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "\n",
        "start = time()\n",
        "for epoch_idx in range(num_epochs):\n",
        "    sum_loss = 0.0\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        x = x.float().to(device)\n",
        "        with torch.no_grad():\n",
        "            im, _ = vae.encode(x)\n",
        "\n",
        "        class_condition = torch.nn.functional.one_hot(y, num_classes).to(device)\n",
        "        class_drop_mask = torch.zeros((im.shape[0], 1), device=im.device).float().uniform_(0, 1) > 0.1\n",
        "        y = class_condition * class_drop_mask\n",
        "\n",
        "        t = torch.randint(0, num_timesteps, (im.shape[0],)).to(device)\n",
        "\n",
        "        loss = p_losses(model, im, t, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sum_loss += loss.item()\n",
        "\n",
        "    print(\"epoch:{}, Loss:{}, elapsed time: {}\".format(epoch_idx, sum_loss / len(train_loader), time() - start))\n",
        "    torch.save(model.state_dict(), os.path.join('./ckpt/ddpm_ckpt_class_cond.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "283d123b-08e2-4c9f-afdb-1cf7809f56a6",
      "metadata": {
        "id": "283d123b-08e2-4c9f-afdb-1cf7809f56a6"
      },
      "source": [
        "## 学習済みモデルを用いたノイズからの画像生成とデノイジングの可視化\n",
        "先ほど学習した重みパラメータを用いて，ノイズから画像の生成をします．VQ-VAEの潜在変数サイズと同じランダムなノイズを作成し，その値と生成したい画像のラベルをモデルに入力した結果を確認します．このとき，各ステップの潜在変数をVQ-VAEのDecoderで再構成することでデノイジングを可視化します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "862502b0-aaa1-4b3c-bb58-274486060f5c",
      "metadata": {
        "id": "862502b0-aaa1-4b3c-bb58-274486060f5c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 生成したい画像の条件（MNISTのラベル）\n",
        "sample_class = [0, 1, 2, 3, 4]\n",
        "\n",
        "model = Unet(im_channels=3).to(device)\n",
        "model.eval()\n",
        "model.load_state_dict(torch.load(os.path.join('./ckpt/ddpm_ckpt_class_cond.pth'), map_location=device))\n",
        "\n",
        "vae = VQVAE(im_channels=image_channels).to(device)\n",
        "vae.eval()\n",
        "vae.load_state_dict(torch.load(os.path.join('./ckpt/vqvae_autoencoder_ckpt.pth'), map_location=device), strict=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    sample_classe = torch.tensor(sample_class)\n",
        "    print('Generating images for {}'.format(list(sample_classe.numpy())))\n",
        "    cond_input = torch.nn.functional.one_hot(sample_classe, num_classes).to(device)\n",
        "\n",
        "    # ノイズからのサンプリング\n",
        "    samples = sample(model, vae, cond_input, image_size=7, batch_size=cond_input.size(0), channels=3)\n",
        "\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "combined_list = []\n",
        "\n",
        "for v in samples:\n",
        "    combined_sample = torch.cat([v[i] for i in range(v.shape[0])], dim=2)\n",
        "    combined_list.append(combined_sample)\n",
        "\n",
        "for i in range(num_timesteps):\n",
        "    if i % 50 == 0 or i == 999:\n",
        "        input_imgs = np.concatenate([samples[i][m].reshape(image_size, image_size) for m in range(5)], axis=1)\n",
        "        im = plt.imshow(input_imgs, cmap=\"gray\", animated=True)\n",
        "        ims.append([im])\n",
        "\n",
        "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
        "HTML(animate.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bff7b88-ace4-4067-8ffa-01753d84a50e",
      "metadata": {
        "id": "4bff7b88-ace4-4067-8ffa-01753d84a50e",
        "tags": []
      },
      "source": [
        "# おまけ\n",
        "## Stable Diffusion\n",
        "Stable Diffusionは，LDMの条件付けEncoderと学習データを変更したText-to-Imageモデルです．テキストを条件として画像生成する場合，LDMの論文ではTransformerにより条件付け特徴量を獲得していました．また，学習データセットには画像とテキストのペアで構成されるLAION-400Mが使用されていました．しかし，Stable Diffusionでは条件付けEncoderにCLIPのテキストEncoderを使用します．また，学習データセットにはLAION-400Mよりも大規模であるLAION-5Bのサブセットを使用します．\n",
        "\n",
        "Stable Diffusionがテキストを条件とした場合どのような画像を生成するか，公開されている重みと推論コードを使用して確認します．\n",
        "\n",
        "※ Stable Diffusionには複数のバージョンが存在し，それぞれで条件付けEncoderや学習データセット，モデル構造などが異なります．本ノートブックではStable Diffusion v1を使用しています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf6764e2-6ec0-4b60-933e-6a953c6d1188",
      "metadata": {
        "id": "bf6764e2-6ec0-4b60-933e-6a953c6d1188"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147e065a-94c2-4e1f-a4ed-d30a57a80918",
      "metadata": {
        "id": "147e065a-94c2-4e1f-a4ed-d30a57a80918"
      },
      "outputs": [],
      "source": [
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "\t\"CompVis/stable-diffusion-v1-4\",\n",
        "\tuse_auth_token=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "# 入力するテキスト\n",
        "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
        "with autocast(\"cuda\"):\n",
        "    image = pipe(prompt)[0]\n",
        "\n",
        "image[0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
